{
  "Foundational Principles": { "quote": "Chain-of-thought prompting enables models to solve complex problems by decomposing them into intermediate reasoning steps. (Wei et al., 2022)" },
  "Zero-Shot CoT": { "quote": "Adding 'Let’s think step by step' to prompts elicits structured reasoning without examples: 'Q: {problem} A: Let’s think step by step...'" },
  "Few-Shot CoT": { "quote": "Providing step-by-step exemplars improves generalization: 'Example: Problem → Step 1 → Step 2 → Answer' (Kojima et al., 2022)." },
  "Self-Consistency": { "quote": "Sampling multiple reasoning paths and selecting the most frequent answer improves accuracy. (Wang et al., 2022)" },
  "Problem Decomposition": { "quote": "Divide-and-conquer prompting: 'First, identify subproblems: 1. X, 2. Y... Then solve iteratively.'" },
  "Intermediate Guidance": { "quote": "Guiding partial steps: 'After calculating X, what variables remain? Focus on Y next.'" },
  "Error Analysis & Revision": { "quote": "'Verify each step: Is Step 2 consistent with Step 1? Revise if contradictory.'" },
  "Symbolic-Connectionist Fusion": { "quote": "Combining neural generation with rule-based constraints: 'If X > 5 (rule), then... (LLM continuation).'" },
  "External Knowledge Integration": { "quote": "Augmenting prompts with retrieved facts: 'Given [DB entry: F=ma], explain...'" },
  "Multi-Agent Debate": { "quote": "Simulating collaborative reasoning: 'Agent 1 argues X, Agent 2 critiques → Synthesize consensus.'" },
  "Mathematical CoT": { "quote": "For arithmetic: 'Step 1: 3x + 5 = 20 → Step 2: 3x = 15 → Step 3: x = 5.'" },
  "Causal Reasoning": { "quote": "'Identify root causes: A → B because... Mitigate via C.'" },
  "Temporal Reasoning": { "quote": "Ordering events: 'If X happens before Y, and Y triggers Z, then...'" },
  "Analogical CoT": { "quote": "'This problem resembles [analogy], so apply similar steps: 1. Map variables, 2. Adjust for differences...'" },
  "Socratic Prompting": { "quote": "'What assumptions underlie Step 2? How does Evidence A support Conclusion B?'" },
  "Meta-Prompts": { "quote": "'Critique this reasoning chain: Are there leaps in logic? Missing steps?'" },
  "CoT for Code": { "quote": "Pseudocode-first: 'Step 1: Loop through array. Step 2: Track max value... → Translate to Python.'" },
  "Visual CoT": { "quote": "Combining text and diagrams: 'Sketch a flowchart, then describe transitions.'" },
  "Uncertainty-Aware CoT": { "quote": "'Assign confidence scores: Step 1 (90%), Step 2 (75%) → Flag low-certainty steps.'" },
  "Ethical Scaffolding": { "quote": "'Before concluding, assess biases: Could Step 3 disproportionately impact Group X?'" },
  "Recursive Refinement": { "quote": "Iterative prompting: 'Revise Step 4 using feedback from Step 3’s output.'" },
  "Domain-Specialized CoT": { "quote": "Medical diagnosis: 'Rule out [condition] first via [test], then evaluate [symptom].'" },
  "Cognitive Mimicry": { "quote": "Emulate expert problem-solving: 'How would a mathematician approach this proof?'" },
  "CoT Evaluation Metrics": { "quote": "Score reasoning chains via faithfulness (logic alignment) and completeness (step coverage)." },
  "Latent CoT": { "quote": "Implicit step elicitation: 'The model internally infers unstated reasoning transitions.'" },
  "Cross-Model CoT": { "quote": "LLM generates steps → Symbolic solver computes results (e.g., WolframAlpha integration)." },
  "Resource Constraints": { "quote": "Balancing depth vs. token limits: 'Truncate redundant steps without losing logic.'" },
  "Human-in-the-Loop CoT": { "quote": "Hybrid systems: 'Model proposes steps → Human edits Step 3 → Model revises output.'" },
  "Scalability Challenges": { "quote": "Long chains risk coherence loss: 'Segment into subtask chains with checkpoints.'" },
  "Future Directions": { "quote": "Automated CoT optimization: Using RL to refine prompting strategies (Yao et al., 2023)." }
}
