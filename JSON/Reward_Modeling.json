{  
  "Reward Function Design": { "quote": "Define R(s,a) → scalar reward to incentivize desired behavior (e.g., R = +1 for goal states)." },  
  "Inverse Reinforcement Learning": { "quote": "Infer R(s,a) from expert demonstrations: maxₐ E[ΣγᵗR(sₜ)] (Ng & Russell, 2000)." },  
  "Human-in-the-Loop Feedback": { "quote": "RLHF: Train reward models using human preferences (e.g., ChatGPT’s Proximal Policy Optimization)." },  
  "Reward Hacking": { "quote": "Adversarial loopholes: Agent maximizes R by unintended means (e.g., CoastRunners boat circling)." },  
  "Ethical Alignment": { "quote": "Avoid harmful reward shortcuts: E.g., 'paperclip maximizer' thought experiment (Bostrom)." },  
  "Multi-Objective Optimization": { "quote": "Pareto fronts: Balance competing goals (e.g., speed vs. safety) with λ₁R₁ + λ₂R₂." },  
  "Reward Shaping": { "quote": "Add potential-based shaping: R’ = R + γΦ(s’) - Φ(s) to guide exploration (Ng et al., 1999)." },  
  "Preference Learning": { "quote": "Bradley-Terry model: P(a ≻ b) = σ(R(a) - R(b)) for pairwise comparisons." },  
  "Intrinsic Motivation": { "quote": "Curiosity-driven rewards: R = ||f(sₜ₊₁) - f(sₜ)||² (prediction error as exploration bonus)." },  
  "Adversarial Reward Modeling": { "quote": "Minimax robustness: min_θ max_δ L(Rθ, δ) → defend against reward tampering." },  
  "Behavioral Cloning": { "quote": "Mimic expert actions: Unlike IRL, no explicit reward inference (supervised learning)." },  
  "Exploration vs. Exploitation": { "quote": "ε-greedy: Trade off between maximizing known rewards (ε=0) vs. seeking new ones (ε=1)." },  
  "Safety Constraints": { "quote": "Hard/soft constraints: Lagrangian penalties L = R - λC (C ≤ 0 for safe actions)." },  
  "Transferable Rewards": { "quote": "Meta-learning R(s,a; φ) → adapt φ to new tasks via few-shot human feedback." },  
  "Scalable Annotation": { "quote": "Synthetic feedback: Train reward models on GPT-4-generated preferences (Scaled RLHF)." },  
  "Bias Mitigation": { "quote": "Debias human feedback: Re-weight annotations to correct for skewed preferences." },  
  "Temporal Credit Assignment": { "quote": "Distributed rewards: Use eligibility traces or attention to attribute long-term outcomes." },  
  "Proxy Metrics": { "quote": "Optimizing ≠ aligning: YouTube’s watch time vs. user satisfaction (Leike et al., 2018)." },  
  "Causal Rewards": { "quote": "Counterfactual reasoning: R = E[Y|do(a)] - E[Y|do(¬a)] (Pearl’s do-calculus)." },  
  "Reward Uncertainty": { "quote": "Bayesian R models: Maintain posterior P(R|D) to handle ambiguous human feedback." },  
  "Multi-Agent Incentives": { "quote": "Mechanism design: Nash equilibria in games where agents optimize R₁, R₂, ..., Rₙ." },  
  "Interpretability": { "quote": "Saliency maps: Highlight which states/actions most influenced R(s,a) predictions." },  
  "Real-World Deployment": { "quote": "Sim-to-real gaps: Calibrate R(s,a) using domain randomization (OpenAI’s Dactyl)." },  
  "Regulatory Compliance": { "quote": "GDPR/Algorithmic Accountability: Audit trails for reward model decisions." },  
  "Future Directions": { "quote": "Neuro-symbolic rewards: Combine LLM-generated rules with neural preference models." }  
}  
