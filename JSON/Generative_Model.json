{
  "Generative Adversarial Networks (GANs)": { 
    "quote": "Minimax game: min_G max_D V(D, G) = ğ”¼[log D(x)] + ğ”¼[log(1 - D(G(z)))]. (Goodfellow et al., 2014)"
  },
  "Variational Autoencoders (VAEs)": { 
    "quote": "Maximizing ELBO: ğ”¼[log p(x|z)] - D_KL(q(z|x) || p(z)), where q is the approximate posterior. (Kingma & Welling, 2013)"
  },
  "Autoregressive Models (e.g., PixelRNN)": { 
    "quote": "Chain rule for sequences: p(x) = âˆ p(x_i | x_1, ..., x_{i-1}). (van den Oord et al., 2016)"
  },
  "Normalizing Flows": { 
    "quote": "Invertible transformations: log p(x) = log p(z) + log |det âˆ‚f^{-1}(x)/âˆ‚x |. (Rezende & Mohamed, 2015)"
  },
  "Diffusion Models": { 
    "quote": "Gradual denoising: q(x_t | x_{t-1}) = ğ’©(x_t; âˆš(1-Î²_t)x_{t-1}, Î²_tğˆ). (Sohl-Dickstein et al., 2015)"
  },
  "Energy-Based Models (EBMs)": { 
    "quote": "Unnormalized density: p(x) = exp(-E(x))/Z, where Z is the partition function. (LeCun et al., 2006)"
  },
  "Boltzmann Machines": { 
    "quote": "Stochastic neurons: p(x) = exp(-E(x))/Z, E(x) = -âˆ‘W_ij x_i x_j. (Hinton & Sejnowski, 1986)"
  },
  "Generative Pre-trained Transformers (GPT)": { 
    "quote": "Autoregressive language modeling: p(x) = âˆ p(x_i | x_{<i}; Î¸). (Radford et al., 2018)",
    "type": "Semi-Supervised"
  },
  "Moment Matching Networks": { 
    "quote": "Matching moments: ğ”¼[Ï•(x)] â‰ˆ ğ”¼[Ï•(G(z))]. (Li et al., 2015)"
  },
  "Adversarial Autoencoders": { 
    "quote": "Latent space alignment: min_G max_D ğ”¼[log D(z)] + ğ”¼[log(1 - D(G(x)))]. (Makhzani et al., 2015)"
  }
}
