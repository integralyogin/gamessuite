{
  "Reactive Agents": { "quote": "Stimulus → Response: No internal state (e.g., Brooks' Subsumption Architecture: AvoidCollision() > Wander())." },
  "Belief-Desire-Intention (BDI)": { "quote": "Agent = B + D + I. BDI logic formalizes reasoning (Rao & Georgeff, 1995)." },
  "Utility-Based Agents": { "quote": "Maximize utility: argmax_a Σ P(outcome | a) * Utility(outcome)." },
  "Learning Agents": { "quote": "Reinforcement Learning: Q(s,a) ← Q(s,a) + α[r + γmax Q(s',a')] (Bellman Equation)." },
  "Multi-Agent Systems": { "quote": "Game Theory: Nash Equilibrium (no incentive to deviate) → Auctions, cooperation (e.g., RoboCup)." },
  "Hierarchical Agents": { "quote": "HTN Planning: Decompose tasks (e.g., STRIPS → PDDL for robot navigation)." },
  "Hybrid Architectures": { "quote": "Subsumption + Symbolic Planning: Layered reactive/deliberative control (e.g., Rodney Brooks' Cog)." },
  "Cognitive Architectures": { "quote": "SOAR/ACT-R: Simulate human cognition via production rules (If condition Then action)." },
  "Planner-Based Agents": { "quote": "STRIPS operators: Preconditions → Effects (e.g., A* for pathfinding)." },
  "Embodied Agents": { "quote": "Situated AI: Perception-action loops grounded in environment (e.g., Rodney Brooks' *nouvelle AI*)." },
  "Decentralized Swarm Agents": { "quote": "Stigmergy: Ant Colony Optimization (ACO) → Pheromone trails for pathfinding." },
  "Transformer-Based Agents": { "quote": "Attention is all you need: Agent(observation) = softmax(QKᵀ/√d) V (Vaswani et al., 2017)." },
  "Neuro-Symbolic Agents": { "quote": "Fuse neural nets + logic: NeuralModule(image) → SymbolicReasoner(facts)." },
  "Ethical Agents": { "quote": "Value alignment: Constrain actions via Kantian norms (e.g., Asimov's Laws of Robotics)." },
  "Explainable Agents": { "quote": "Generate post-hoc rationales: SHAP/LIME for transparency (e.g., medical diagnosis bots)." },
  "Quantum Agents": { "quote": "Grover's algorithm: O(√N) search speedup for decision-making (e.g., optimal resource allocation)." },
  "Federated Learning Agents": { "quote": "Decentralized training: Δθ = ClientUpdate(θ, local_data) → SecureAggregate(Δθ₁, Δθ₂...)." },
  "Autonomous Vehicle Agents": { "quote": "Sense → Plan → Act: LiDAR + SLAM → Trajectory optimization (e.g., Waymo's motion planning)." },
  "Dialog Agents": { "quote": "GPT-3.5: Next-token prediction → Chain-of-Thought prompting for reasoning." },
  "Self-Improving Agents": { "quote": "Recursive Self-Improvement: Agent → Modify(Agent) (e.g., Gödel Machine, 2003)." },
  "Affective Agents": { "quote": "Emotion modeling: PAD (Pleasure-Arousal-Dominance) space for social robots." },
  "Agent Frameworks": { "quote": "RLlib (Ray), OpenAI Gym: Agent.train(env, policy='PPO')." },
  "Swarm Intelligence": { "quote": "Boids: Flocking rules (separation, alignment, cohesion) → Emergent behavior." },
  "Formal Verification Agents": { "quote": "Temporal Logic: Ensure safety via model checking (e.g., LTL/CTL constraints)." },
  "Multi-Modal Agents": { "quote": "CLIP (Contrastive Learning): Align text + image embeddings for cross-modal reasoning." },
  "Meta-Learning Agents": { "quote": "MAML: θ' = θ - α∇θ L_task(θ) → Adapt to new tasks with few examples." },
  "Neuromorphic Agents": { "quote": "Spiking Neural Nets: Event-driven processing (e.g., Intel Loihi for energy efficiency)." },
  "Agent Ethics": { "quote": "Moral uncertainty: Maximize expected choice-worthiness (Yudkowsky, 2020)." }
}
