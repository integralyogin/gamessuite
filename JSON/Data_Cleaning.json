{
"Missing Values": { "quote": "Handling data gaps through deletion, imputation, or explicit flagging. df.fillna(method='ffill') carries forward the last valid observation." },
"Outlier Detection": { "quote": "Identifying and addressing extreme values that may distort analysis. Z-scores, IQR methods, and isolation forests help flag anomalous data points." },
"Data Typing": { "quote": "Ensuring variables have appropriate data types for analysis. Converting string '42' to integer 42 enables mathematical operations." },
"Standardization": { "quote": "Rescaling features to have mean=0 and standard deviation=1. Z-score normalization: z = (x - μ) / σ" },
"Normalization": { "quote": "Rescaling features to a common range, typically [0,1]. Min-max scaling: x_norm = (x - min) / (max - min)" },
"Deduplication": { "quote": "Identifying and removing redundant observations. df.drop_duplicates() removes exact matches across all columns." },
"String Cleaning": { "quote": "Standardizing text data through case normalization, whitespace removal, and consistent formatting. 'New York ' and 'new york' become identical after proper cleaning." },
"Categorical Encoding": { "quote": "Converting categorical variables to numeric representations. One-hot encoding transforms 'red', 'green', 'blue' into binary indicator columns." },
"Date Parsing": { "quote": "Converting various date formats into standardized datetime objects. pd.to_datetime() handles multiple formats and timezone normalization." },
"Feature Engineering": { "quote": "Creating new variables from existing data to improve analysis. Extracting day-of-week from timestamps enables temporal pattern discovery." },
"Data Validation": { "quote": "Establishing rules to ensure data meets quality expectations. Setting valid ranges, formats, and relationships between variables." },
"Data Consistency": { "quote": "Addressing contradictions within datasets. Ensuring a customer can't be both 'active' and 'churned' simultaneously." },
"Error Correction": { "quote": "Fixing identifiable mistakes in data collection or entry. Correcting zip codes that don't match city/state combinations." },
"Data Transformation": { "quote": "Applying mathematical functions to improve data properties. Log transformations can normalize skewed distributions: y = log(x)" },
"Handling Imbalanced Data": { "quote": "Addressing disproportionate class distributions. Techniques include oversampling minorities (SMOTE) or undersampling majorities." },
"Feature Selection": { "quote": "Identifying and keeping relevant variables while discarding others. Removing highly correlated features reduces redundancy without losing information." },
"Dimensionality Reduction": { "quote": "Decreasing the number of random variables through transformation. PCA projects data onto lower-dimensional space while preserving variance." },
"Data Integration": { "quote": "Combining data from multiple sources while resolving conflicts. Merging customer records through reliable unique identifiers." },
"Data Restructuring": { "quote": "Changing data layout to facilitate analysis. Converting between wide and long formats using pivot or melt operations." },
"Data Discretization": { "quote": "Converting continuous variables into categorical bins. Age ranges like 18-24, 25-34 simplify analysis for certain applications." },
"Handling Special Values": { "quote": "Standardizing representation of nulls, zeroes, and placeholders. Distinguishing between 0 (measured zero) and NULL (not measured)." },
"Time Series Cleansing": { "quote": "Addressing issues specific to sequential data. Resampling irregular timestamps to consistent intervals with appropriate aggregation." },
"Data Enrichment": { "quote": "Enhancing existing data with external information. Adding geographic coordinates to address data enables spatial analysis." },
"Noise Reduction": { "quote": "Removing random variation that obscures underlying patterns. Moving averages smooth time series data to reveal trends." },
"Quality Assurance": { "quote": "Systematic processes to verify data cleanliness. Automated tests confirm that cleaning operations produced the expected results." },
"Documentation": { "quote": "Recording cleaning steps and decisions for reproducibility. Data dictionaries and transformation logs enable others to understand the process." },
"Data Lineage": { "quote": "Tracking data origins and transformations throughout its lifecycle. Understanding how raw data becomes analysis-ready through each processing step." },
"ETL Processes": { "quote": "Extract, Transform, Load workflows for systematic data preparation. Automating cleaning steps for regular data refreshes." },
"Data Profiling": { "quote": "Examining source data to understand its structure, content, and quality. Summarizing distributions, patterns, and anomalies before cleaning begins." },
"Error Analysis": { "quote": "Investigating patterns in data problems to address root causes. Identifying systematic issues in data collection processes rather than just symptoms." }
}
