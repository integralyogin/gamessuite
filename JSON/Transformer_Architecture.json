{  
  "Attention Mechanism": { "quote": "Scaled Dot-Product Attention: Attention(Q,K,V) = softmax(QKᵀ/√d_k)V (Vaswani et al., 2017)." },  
  "Multi-Head Attention": { "quote": "Parallel attention heads: h_i = Attention(QW_i^Q, KW_i^K, VW_i^V) → Concat(h_1,...,h_n)W^O." },  
  "Positional Encoding": { "quote": "Sine/cosine embeddings: PE(pos,2i) = sin(pos/10000^(2i/d_model)), PE(pos,2i+1) = cos(...)." },  
  "Encoder-Decoder Structure": { "quote": "Encoder: Self-attention → FFN. Decoder: Masked self-attention → Encoder-decoder attention." },  
  "Layer Normalization": { "quote": "Stabilize training: y = (x - μ)/σ * γ + β (applied post-attention & post-FFN)." },  
  "Feed-Forward Networks": { "quote": "Position-wise MLP: FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 (ReLU activation)." },  
  "Self-Attention vs. Cross-Attention": { "quote": "Self: Q=K=V (input sequence). Cross: Q from decoder, K/V from encoder." },  
  "Masked Attention": { "quote": "Decoder masking: M_{i,j} = -∞ if j > i (prevent future token visibility)." },  
  "Transformer Variants": { "quote": "BERT (bidirectional), GPT (autoregressive), T5 (text-to-text), ViT (vision)." },  
  "Efficient Attention": { "quote": "FlashAttention: IO-aware optimization → O(N²) → O(N) with GPU SRAM (Dao et al., 2022)." },  
  "Sparse Attention": { "quote": "Local windows (Longformer) or axial patterns (Sparse Transformer) → reduce O(N²)." },  
  "Pre-training Objectives": { "quote": "BERT: MLM (masked language modeling). GPT: Causal LM (next token prediction)." },  
  "Fine-Tuning": { "quote": "Adapter layers: Freeze pretrained weights → train small FFN inserts (Houlsby et al., 2019)." },  
  "Cross-Modality": { "quote": "CLIP: Contrastive loss aligning text + image embeddings (Radford et al., 2021)." },  
  "Positional Innovations": { "quote": "RoPE (Rotary Position Embedding): x_i → x_i e^{iθ} for relative positions (Su et al., 2021)." },  
  "Training Dynamics": { "quote": "AdamW optimizer: β1=0.9, β2=0.98, ε=1e-9, weight decay λ=0.01 (LLaMA recipe)." },  
  "Loss Functions": { "quote": "Cross-entropy loss: L = -Σ y_i log p_i + label smoothing (ε=0.1)." },  
  "Model Scaling": { "quote": "Chinchilla Laws: N=20D, where N=tokens, D=parameters (Hoffmann et al., 2022)." },  
  "Hardware Constraints": { "quote": "KV caching: O(N) memory per layer → batch size ↓ as sequence length ↑." },  
  "Distillation": { "quote": "DistilBERT: Teacher → Student via KL divergence loss (Sanh et al., 2019)." },  
  "Attention Patterns": { "quote": "Local (sliding window), global (CLS token), or strided (Sparse Transformer)." },  
  "Ethical Considerations": { "quote": "Bias mitigation: Debiasing attention heads (Bolukbasi et al., 2021)." },  
  "Multimodal Transformers": { "quote": "Flamingo: Perceiver Resampler → fuse text + vision tokens (Alayrac et al., 2022)." },  
  "Future Directions": { "quote": "Hybrid architectures: MoE (Mixture of Experts) + Transformers (e.g., Switch Transformer)." }  
}  
