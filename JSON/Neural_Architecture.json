{
"Feedforward Networks": { "quote": "Input → Hidden Layers → Output, with no cycles: classic multilayer perceptron (MLP)" },
"Convolutional Networks": { "quote": "Convolution + Pooling + Dense layers: ideal for spatial data processing" },
"Recurrent Networks": { "quote": "Cycles allow memory of previous inputs: ht = f(ht-1, xt)" },
"LSTM Architecture": { "quote": "Gates control information flow: forget, input, output gates manage cell state" },
"GRU Architecture": { "quote": "Simplified recurrent unit: reset and update gates control memory" },
"Transformer Architecture": { "quote": "Self-attention mechanism: Q×K×V with multiple attention heads" },
"Autoencoder Structure": { "quote": "Encoder → Latent Space → Decoder: dimensionality reduction" },
"Residual Networks": { "quote": "Skip connections: x + F(x) allows deeper architectures" },
"Siamese Networks": { "quote": "Twin networks with shared weights: comparing two inputs" },
"Generative Adversarial": { "quote": "Generator vs Discriminator: min-max game for data generation" },
"Graph Neural Networks": { "quote": "Message passing between nodes: aggregate neighbor information" },
"Capsule Networks": { "quote": "Part-whole relationships: pose matrices between layers" },
"Attention Mechanisms": { "quote": "Weight inputs by relevance: attention_score = softmax(QK^T)V" },
"Memory Networks": { "quote": "External memory access: read/write operations on memory matrix" },
"Deep Belief Networks": { "quote": "Stacked RBMs: layer-wise pretraining" },
"Neural Turing Machines": { "quote": "Differentiable memory access: controller + memory matrix" },
"Echo State Networks": { "quote": "Fixed recurrent layer: only output weights trained" },
"Hopfield Networks": { "quote": "Recurrent energy-based: stable states as memories" },
"Boltzmann Machines": { "quote": "Stochastic neurons: energy-based probabilistic models" },
"Self-Organizing Maps": { "quote": "Competitive learning: topology-preserving mappings" },
"Highway Networks": { "quote": "Gated skip connections: learned information flow control" },
"DenseNet Architecture": { "quote": "Each layer connected to all others: maximum information flow" },
"U-Net Architecture": { "quote": "Encoding path → Decoding path: semantic segmentation" },
"Adaptive Networks": { "quote": "Dynamic architecture: structure evolves during training" },
"Hierarchical Networks": { "quote": "Multi-level organization: compositional feature learning" },
"Multi-Task Networks": { "quote": "Shared features: multiple output heads for different tasks" },
"Meta-Learning Networks": { "quote": "Learning to learn: architecture adapts to new tasks" },
"Neuroevolution": { "quote": "Evolved architecture: topology and weights optimized genetically" },
"Neural ODEs": { "quote": "Continuous depth: network as differential equation" },
"Modular Networks": { "quote": "Specialized subnetworks: composition of expert modules" }
}
