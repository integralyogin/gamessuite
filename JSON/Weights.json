{
  "Weight Initialization": { "quote": "Xavier/Glorot: Var(W) = 2/(n_input + n_output); He: Var(W) = 2/n_input (prevents vanishing/exploding gradients)." },
  "L1 Regularization (Lasso)": { "quote": "Loss += λΣ|w|; induces sparsity: drives insignificant weights to zero." },
  "L2 Regularization (Ridge)": { "quote": "Loss += λΣw²; penalizes large weights: encourages smoothness." },
  "Weight Updates (SGD)": { "quote": "w ← w - η∇J(w); η = learning rate, ∇J = gradient of loss." },
  "Momentum Optimization": { "quote": "v ← βv + (1-β)∇J(w); w ← w - ηv (dampens oscillations)." },
  "Adam Optimizer": { "quote": "m_t = β₁m_{t-1} + (1-β₁)g_t; v_t = β₂v_{t-1} + (1-β₂)g_t²; w ← w - η m̂_t/(√v̂_t + ε)." },
  "Weight Sharing": { "quote": "Convolutional layers reuse weights across spatial locations (translation invariance)." },
  "Weight Decay": { "quote": "Synonym for L2 regularization: ||W||² penalty during optimization." },
  "Vanishing Gradients": { "quote": "∂Loss/∂w ≈ 0 for deep networks (solved via ReLU, residual connections)." },
  "Weight Pruning": { "quote": "Remove |w| < threshold; reduces model size (e.g., lottery ticket hypothesis)." },
  "Bayesian Weights": { "quote": "Treat weights as distributions: p(w|D) ∝ p(D|w)p(w) (MCMC or variational inference)." },
  "Weight Quantization": { "quote": "Store weights as 8-bit integers vs. 32-bit floats: reduces memory (e.g., TensorRT)." },
  "Attention Weights": { "quote": "Softmax(QKᵀ/√d) in transformers: learns contextual importance (Vaswani et al., 2017)." },
  "Weight Interpretation": { "quote": "Linear models: w_i magnitude = feature importance (e.g., logistic regression)." },
  "GAN Weight Balancing": { "quote": "Generator vs. discriminator loss equilibrium (Goodfellow et al., 2014)." },
  "Weight Transfer": { "quote": "Fine-tuning pretrained weights (e.g., ImageNet → medical imaging)." },
  "Recurrent Weights": { "quote": "W_{hh} in RNNs: h_t = σ(W_{hh}h_{t-1} + W_{xh}x_t) (gradient clipping for stability)." },
  "Weight Visualization": { "quote": "CNN filters as edge detectors (Layer 1) → texture/shape (Layer 5)." },
  "Weight Symmetry": { "quote": "Symmetric initialization breaks symmetry for diverse feature learning (e.g., ReLU)." },
  "Weight Entropy": { "quote": "Uncertainty quantification: H(w) = -Σp(w)logp(w) (Bayesian neural networks)." },
  "Adversarial Weights": { "quote": "Fooling models via perturbed inputs: argmax_{δ} Loss(f(x+δ; w), ||δ|| < ε." },
  "Weight Scaling": { "quote": "BatchNorm: γ(wx + β) adjusts dynamic range (accelerates convergence)." },
  "Weight Sparsity": { "quote": "Prune >90% weights without accuracy loss (e.g., sparse transformers)." },
  "Weight Averaging": { "quote": "SWA (Stochastic Weight Averaging): w_swa = 1/k Σ w_t (improves generalization)." },
  "Weight in RL Policies": { "quote": "π(a|s; w): weights define action probabilities (e.g., policy gradients)." },
  "Weight Initialization (VAEs)": { "quote": "Reparameterization trick: z = μ + σ⊙ε, ε ~ N(0, I) (Kingma & Welling, 2013)." },
  "Weight Robustness": { "quote": "Adversarial training: min_w E[max_δ Loss(x+δ, y; w)] (Madry et al., 2018)." },
  "Weight in Embeddings": { "quote": "word2vec: w_ij = P(word_j | word_i) (Mikolov et al., 2013)." },
  "Weight in Dropout": { "quote": "Train: w ← w ⊙ Bernoulli(p); Inference: w ← w × p (Srivastava et al., 2014)." },
  "Weights in Deep Learning": { "quote": "Parameters ≈ weights + biases; modern networks: ~1e8 to ~1e12 weights (GPT-3)." }
}
