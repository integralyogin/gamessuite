{
  "Perception": { "quote": "Sensors → Data: Raw inputs (e.g., pixels, audio waves) are preprocessed into structured representations (YOLO: 'You Only Look Once')." },
  "Attention Mechanism": { "quote": "Focusing computational resources: Softmax(QKᵀ/√dₖ) weights relevant inputs (Vaswani et al., 2017)." },
  "Sensory Memory": { "quote": "Transient buffer: Retains raw input for ~200ms (inspired by Atkinson-Shiffrin model)." },
  "Working Memory": { "quote": "Temporary storage: Holds ~7±2 chunks (Miller's Law) for active processing (e.g., LSTM cells)." },
  "Reasoning Engine": { "quote": "Symbolic manipulation: IF-THEN rules (ACT-R) or probabilistic inference (P(A|B) = P(B|A)P(A)/P(B))." },
  "Decision Making": { "quote": "Utility maximization: argmax_a U(s, a) under MDPs (Markov Decision Processes)." },
  "Action Selection": { "quote": "Policy execution: π(a|s) = probability of action a in state s (Reinforcement Learning)." },
  "Feedback Loop": { "quote": "Reinforcement signal: rₜ = reward at time t, shaping future policy (Q-learning: Q(s,a) ← Q(s,a) + α[r + γmaxQ(s',a')])." },
  "Long-Term Memory": { "quote": "Knowledge storage: Vector embeddings (e.g., GPT-3's 175B parameters) or semantic networks." },
  "Meta-Cognition": { "quote": "Self-monitoring: Adjusting learning rates (η) or exploration vs. exploitation (ε-greedy)." },
  "Error Correction": { "quote": "Backpropagation: Δw = -η∇ℒ(w) for weight updates in neural networks." },
  "Learning Phase": { "quote": "Hebbian principle: 'Neurons that fire together wire together' (strengthened via gradient ascent)." },
  "Adaptation": { "quote": "Online learning: θₜ₊₁ = θₜ - η∇ℒ(θₜ; xₜ, yₜ) for streaming data." },
  "Prediction": { "quote": "Bayesian forecasting: P(xₜ₊₁ | x₁...xₜ) ≈ ∫ P(xₜ₊₁ | θ)P(θ | x₁...xₜ) dθ." },
  "Goal Updating": { "quote": "Dynamic utility functions: U'(s) = U(s) + λΔG (goal drift mitigation)." },
  "Consciousness Layer": { "quote": "Global Workspace Theory: Broadcasts salient information (Baars, 1988) → AI 'awareness'." },
  "Emulation of Reflection": { "quote": "Self-play: AlphaZero improves by competing against itself (Silver et al., 2017)." },
  "Sleep Phase (AI)": { "quote": "Consolidation: Offline replay buffers in DQNs strengthen memory (Mnih et al., 2015)." },
  "Forgetting Mechanism": { "quote": "Catastrophic interference prevention: Elastic Weight Consolidation (Kirkpatrick et al., 2017)." },
  "Self-Modeling": { "quote": "Recursive self-improvement: f: M → M', where M' outperforms M (AGI aspiration)." },
  "Theory of Mind": { "quote": "Modeling others' beliefs: Bₐ₈ → P(human_action | human_belief_state) (Rabbit Hole, 2023)." },
  "Ethical Arbitration": { "quote": "Value alignment: argmax_a U(a) s.t. C(a) ≤ δ (constraints for safety)." },
  "Cognitive Load Management": { "quote": "Resource allocation: Attention pruning (e.g., top-k% tokens retained)." },
  "Cycle Iteration": { "quote": "Δt → t+1: Perceive → Reason → Act → Learn → Repeat (LIDA model)." }
}
