{
  "Retrieval-Augmented Generation": { "quote": "Augment prompts with retrieved knowledge: p(y|x, D_retrieved) ∝ p(y|x) * p(D_retrieved|x,y)." },
  "Few-Shot Basics": { "quote": "Guide models with examples: [Input1 → Output1, ..., InputN → OutputN] → Predict OutputN+1." },
  "Hybrid Retrieval-Prompting": { "quote": "Retrieve → Filter → Inject: RAG pipelines combine dense retrieval (e.g., FAISS) with GPT-style generation." },
  "Dynamic Context Selection": { "quote": "Top-k passages from corpus D: argmax_{d∈D} sim(q, d) where q = embed(input)." },
  "Prompt Templates": { "quote": "Structured placeholders: 'Answer based on {retrieved_text}: {question} → {answer}'." },
  "Semantic Search": { "quote": "Dense retrieval via SBERT: cosine_similarity(embed(query), embed(document)) > θ." },
  "Example Curation": { "quote": "Select diverse, representative shots: maximize coverage(min, max, std of embeddings)." },
  "Calibration Prompts": { "quote": "Reduce bias: 'Even if unsure, answer factually: {question} →'." },
  "Iterative Retrieval": { "quote": "Query reformulation loops: q_i+1 = LM(q_i, retrieved_docs[i])." },
  "Domain Adaptation": { "quote": "Retrieve in-domain examples: D_subset = {d ∈ D | domain(d) = target}." },
  "Unsupervised Retrieval": { "quote": "Clustering-based selection: k-means(embed(examples)) → centroid examples as shots." },
  "Meta-Prompts": { "quote": "Guiding model behavior: 'You are a historian. Use archaic terms: {query} →'." },
  "Contrastive Examples": { "quote": "Include counterfactuals: 'Q: Can birds fly? A: Most yes, but penguins cannot.'" },
  "Efficiency Optimization": { "quote": "Compress retrieved context: distil(retrieved_text) → retain 95% info." },
  "Cross-Lingual Retrieval": { "quote": "Translate-retrieve-translate: EN → FR → search D_FR → FR → EN." },
  "Multi-Hop Retrieval": { "quote": "Chain retrievals: Answer(Question, Doc1) → Doc2 = retrieve(Answer(Doc1)) → Final Answer." },
  "Uncertainty-Aware Prompts": { "quote": "Model confidence scoring: 'Answer (confidence %): {answer} [85%]'." },
  "Bias Mitigation": { "quote": "Debias retrieved content: filter(D, [toxicity, gender, race] < threshold)." },
  "Evaluation Metrics": { "quote": "Measure with ROUGE-L, BLEU, or precision@k for retrieval quality." },
  "Human-in-the-Loop": { "quote": "Active learning: Flag low-confidence outputs for human annotation → retrain." },
  "Zero-to-Few-Shot": { "quote": "Bootstrapping: Use retrieved examples as synthetic few-shot prompts." },
  "Structured Knowledge Injection": { "quote": "Retrieve triples: (subject, predicate, object) → linearize into prompts." },
  "Temporal Retrieval": { "quote": "Time-aware search: prioritize docs from [2020-2023] for current events." },
  "Privacy-Preserving Retrieval": { "quote": "Federated retrieval: aggregate user-level data without raw access." },
  "Multimodal Retrieval": { "quote": "Text + image search: CLIP embeddings → retrieve hybrid content." },
  "Error Analysis Prompts": { "quote": "'Correct this answer: {model_output} → Revised:' forces self-reflection." },
  "Instructional Prompts": { "quote": "Explicit task framing: 'Summarize this in 10 words: {text} →'." },
  "Chain-of-Thought Retrieval": { "quote": "Retrieve reasoning steps: 'Step1: ..., Step2: ... → Final Answer'." },
  "Feedback Integration": { "quote": "Reinforce with upvotes: fine-tune LM on (retrieved_context + high-scoring outputs)." },
  "Contextual Retrieval": { "quote": "Session-aware: cache prior turns → retrieve based on dialogue history." }
}
