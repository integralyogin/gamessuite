{
  "Definition": { "quote": "Embeddings map high-dimensional data to lower-dimensional spaces while preserving relationships: ℝⁿ → ℝᵏ (k ≪ n)." },
  "Word Embeddings": { "quote": "Word2Vec: Skip-gram maximizes log P(wₜ₊ⱼ | wₜ) for context window j (Mikolov et al., 2013)." },
  "Sentence Embeddings": { "quote": "BERT [CLS] token: [CLS] → ℝ⁷⁶⁸ captures sentence semantics (Devlin et al., 2018)." },
  "Document Embeddings": { "quote": "Doc2Vec: Paragraph vector d concatenated with word vectors (Le & Mikolov, 2014)." },
  "Graph Embeddings": { "quote": "Node2Vec: Biased random walks preserve network neighborhoods (Grover & Leskovec, 2016)." },
  "Image Embeddings": { "quote": "CNNs: Pooling layer outputs (e.g., ResNet-50 → ℝ²⁰⁴⁸) encode visual features." },
  "Dimensionality Reduction": { "quote": "Preserve structure: PCA maximizes variance, t-SNE preserves local similarities." },
  "PCA": { "quote": "Eigen-decomposition: XᵀX = VΛVᵀ, embedding = XVₖ (top k eigenvectors)." },
  "t-SNE": { "quote": "Minimizes KL divergence between high-dim P and low-dim Q: KL(P || Q) = Σ pᵢⱼ log(pᵢⱼ/qᵢⱼ)." },
  "UMAP": { "quote": "Optimizes fuzzy topological structure: Cross-entropy loss for simplicial sets." },
  "Embedding Sparsity": { "quote": "L1 regularization: Loss += λ||θ||₁ to encourage sparse representations." },
  "Embedding Similarity": { "quote": "Cosine similarity: cos(θ) = (A·B)/(||A|| ||B||) ∈ [-1, 1]." },
  "Embedding Visualization": { "quote": "Projection tools: TensorBoard PCA/UMAP for ℝᵏ → ℝ²/ℝ³." },
  "Transfer Learning": { "quote": "Pre-trained embeddings (e.g., GloVe) → fine-tuned for downstream tasks." },
  "Contextual Embeddings": { "quote": "Transformer outputs: Position-dependent embeddings (e.g., BERT, GPT)." },
  "Static vs. Dynamic": { "quote": "Static (GloVe) vs. Contextual (ELMo): \"I bank on the river\" vs. \"bank account\"." },
  "Embedding Training": { "quote": "Negative sampling: Approximate softmax with log σ(v⋅vₙₑ₉) for efficiency." },
  "Embedding Evaluation": { "quote": "Analogy tasks: king - man + woman ≈ queen (accuracy@k)." },
  "Multimodal Embeddings": { "quote": "CLIP: Contrastive loss aligns text-image pairs (Radford et al., 2021)." },
  "Embedding Bias": { "quote": "Debiasing: Subtract gender subspace (Bolukbasi et al., 2016)." },
  "Quantization": { "quote": "Product quantization: Split ℝᵏ into m subspaces → compress embeddings." },
  "Embedding Layers": { "quote": "Framework tools: tf.keras.layers.Embedding(input_dim, output_dim)." },
  "Contrastive Learning": { "quote": "SimCLR: NT-Xent loss maximizes agreement between augmented views." },
  "Positional Embeddings": { "quote": "Sinusoidal encoding: PE(pos,2i) = sin(pos/10000²ⁱ/ᵈ), PE(pos,2i+1) = cos(...)." },
  "Embedding Storage": { "quote": "FAISS: Efficient similarity search and clustering of dense vectors (Meta AI)." },
  "Embedding Applications": { "quote": "Semantic search: Query ≈ Nearest neighbors in embedding space." },
  "Embedding in Recommenders": { "quote": "Collaborative filtering: User/item embeddings → dot product predicts affinity." },
  "Embedding Ethics": { "quote": "Fairness: Mitigate racial/gender bias in word embeddings (e.g., debiased GloVe)." },
  "Embedding for Clustering": { "quote": "K-means on embeddings: Assign clusters via argmin||xᵢ - μⱼ||²." },
  "Embedding Normalization": { "quote": "L2-normalize: x̂ = x/||x|| ensures ||x̂|| = 1 for stable similarity." }
}
