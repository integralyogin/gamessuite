{  
  "Classical Planning": { "quote": "STRIPS: Preconditions → Effects. Domain: (P, A, γ), where γ is the goal state (Fikes & Nilsson, 1971)." },  
  "PDDL": { "quote": "Planning Domain Definition Language: (:action move :parameters (?x ?y) :precondition (at ?x) ...)." },  
  "Heuristic Search": { "quote": "h(n) = relaxed plan cost. A* with f(n) = g(n) + h(n) → Optimal plan extraction." },  
  "Hierarchical Planning (HTN)": { "quote": "Decompose tasks: CompoundTask → Subtasks (e.g., navigate → [plan_route, avoid_obstacles])." },  
  "Markov Decision Processes (MDP)": { "quote": "V(s) = maxₐ [R(s,a) + γΣₛ’ T(s,a,s’)V(s’)] → Solve via Value Iteration (Bellman Equation)." },  
  "Partially Observable MDP (POMDP)": { "quote": "Belief states: b’(s’) = η O(o|s’,a) Σₛ T(s,a,s’) b(s). Intractable for large |S|." },  
  "Temporal Planning": { "quote": "Simple Temporal Networks: Constraints (e.g., end(A) ≤ start(B) – 5) → STP solver." },  
  "Graphplan": { "quote": "Planning-graph layers: Mutex actions → Extract parallelizable plans (Blum & Furst, 1995)." },  
  "SAT-based Planning": { "quote": "Encode planning as Boolean satisfiability: ∃ sequence → SAT solver (e.g., MiniSAT)." },  
  "Non-Deterministic Planning": { "quote": "Contingent plans: if (sensor_input = obstacle) → replan() else → proceed()." },  
  "Goal Recognition": { "quote": "Infer agent’s goal from observations: argmax_g P(g|O) using plan library priors." },  
  "Multi-Agent Planning": { "quote": "Nash equilibrium: Joint plan where no agent benefits by deviating (e.g., traffic coordination)." },  
  "Task and Motion Planning (TAMP)": { "quote": "Integrate symbolic (open(door)) with geometric (robot trajectory) planning." },  
  "Reinforcement Learning for Planning": { "quote": "Q-learning: Q(s,a) ← Q(s,a) + α[r + γmaxₐ’ Q(s’,a’) - Q(s,a)] → Policy extraction." },  
  "Plan Execution & Monitoring": { "quote": "Plan repair: Replan if |current_state – expected_state| > ε (e.g., ROSPlan)." },  
  "Automated Theorem Proving": { "quote": "Situational Calculus: Do(move(A,B), S) → Result(move(A,B), S’." },  
  "Probabilistic Planning": { "quote": "POMCP: Monte Carlo Tree Search (MCTS) over belief states (Silver & Veness, 2010)." },  
  "Domain-Specific Planners": { "quote": "FastDownward (IPC winner): Translate → Search (e.g., LM-Cut heuristics)." },  
  "Plan Explanation": { "quote": "Generate human-readable justifications: ‘Action A avoids B due to constraint C’." },  
  "Ethical Planning": { "quote": "Fairness constraints: ∀a ∈ agents, utility(a) ≥ threshold in resource allocation." },  
  "Real-Time Planning": { "quote": "Anytime algorithms: Return best plan found within Δt (e.g., RRT* for robotics)." },  
  "Planning with ML Integration": { "quote": "Transformers for plan generation: Input: Goal(‘cook_meal’) → Output: [take_pan, heat_oil, ...]." },  
  "Plan Validation": { "quote": "Model checking: Verify plan satisfies LTL (Linear Temporal Logic) constraints." },  
  "Adversarial Planning": { "quote": "Game-theoretic minimax: Plan assuming opponent’s worst-case response (e.g., chess AI)." },  
  "Quantum Planning": { "quote": "Grover’s algorithm: O(√N) search for valid plans in unstructured spaces." },  
  "Human-AI Collaborative Planning": { "quote": "Mixed-initiative systems: Human feedback → revise plan (e.g., NASA’s EUROPA)." }  
}  
