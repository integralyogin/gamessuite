{
  "Transformer Architecture": { "quote": "Attention is All You Need (Vaswani et al., 2017): Encoder-Decoder + Self-Attention." },
  "Self-Attention": { "quote": "Q, K, V matrices: Attention(Q,K,V) = softmax(QKᵀ/√d_k)V → Captures token relationships." },
  "Multi-Head Attention": { "quote": "h parallel attention heads: Concat(head₁, ..., headₙ)Wᴼ → Diverse feature learning." },
  "Positional Encoding": { "quote": "Sine/cosine waves: PE(pos,2i) = sin(pos/10000^(2i/d)), PE(pos,2i+1) = cos(...)." },
  "Masked Attention": { "quote": "Decoder causality: Upper-triangular mask (-∞) → Prevents future token peeking." },
  "Feed-Forward Network": { "quote": "Position-wise MLP: FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂ → Nonlinear transformation." },
  "Layer Normalization": { "quote": "Stabilize training: LN(x) = γ(x−μ)/√(σ²+ε) + β → Applied pre-residual connection." },
  "Pre-training Objectives": { "quote": "BERT: Masked Language Modeling (15% tokens masked). GPT: Autoregressive next-token prediction." },
  "Transfer Learning": { "quote": "Fine-tuning: BERT → Sentiment Analysis via [CLS] token. Example: HuggingFace `AutoModel`." },
  "Model Variations": { "quote": "Encoder-only (BERT), Decoder-only (GPT), Encoder-Decoder (T5). Param scale: 175B (GPT-3)." },
  "Efficiency Techniques": { "quote": "Sparse Attention (Longformer), Linear Transformers (kernel ≈ QKᵀ)." },
  "Scaled Dot-Product": { "quote": "√d_k scaling: Mitigates softmax saturation in high-dimensional QKᵀ." },
  "Cross-Attention": { "quote": "Encoder-Decoder bridge: Queries (decoder) attend to Keys/Values (encoder)." },
  "Tokenization": { "quote": "Byte-Pair Encoding (BPE): Subword units → Merge('un', '##able') = 'unable'." },
  "Beam Search": { "quote": "Decoding: Top-k candidates (beam width=5) → Minimize perplexity." },
  "Attention Visualization": { "quote": "Interpretability: Head 3 in layer 5 attends to subject-verb agreement (BERTology)." },
  "Distillation": { "quote": "TinyBERT: Soft labels from BERT-base → 4x compression (Hinton et al.)." },
  "Sparse Transformers": { "quote": "O(n√n) complexity: Local + strided attention (Child et al., 2019)." },
  "Vision Transformers": { "quote": "ViT: Split image into 16x16 patches → Linear embedding → Transformer (Dosovitskiy et al.)." },
  "Multilingual Models": { "quote": "mBERT: Shared subword space → Cross-lingual transfer (104 languages)." },
  "Ethical Considerations": { "quote": "Bias mitigation: Debiasing word embeddings (Bolukbasi et al.) → Fair-CoT." },
  "Memory Optimization": { "quote": "Gradient Checkpointing: Trade compute for memory (Chen et al., 2016)." },
  "Hardware Acceleration": { "quote": "TPU/GPU optimizations: FlashAttention (Dao et al.) → 2x speedup." },
  "Mixture of Experts": { "quote": "Switch Transformers: Routed to expert subsets → 1T params with sparsity." },
  "Retrieval-Augmented": { "quote": "RETRO: External memory bank → k-nearest neighbors during inference." },
  "Quantization": { "quote": "8-bit weights (LLM.int8()) → 4x memory reduction (Dettmers et al.)." },
  "Scaling Laws": { "quote": "Chinchilla: Optimal training with 20B tokens for 70B params (Hoffmann et al., 2022)." }
}
