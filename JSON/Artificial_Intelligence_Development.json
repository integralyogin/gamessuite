{  
  "Machine Learning Basics": { "quote": "Supervised learning: Minimize loss L(θ) = Σ(y_i - ŷ_i)² + λ||θ||² (L2 regularization)." },  
  "Neural Networks": { "quote": "Backpropagation: ∇θJ(θ) via chain rule. Activation: ReLU(x) = max(0, x)." },  
  "Deep Learning": { "quote": "Hierarchical features: CNNs (Conv2D filters), Transformers (self-attention)." },  
  "Natural Language Processing": { "quote": "Tokenization → Embeddings (Word2Vec) → Seq2Seq (LSTM/Transformer)." },  
  "Computer Vision": { "quote": "YOLO: Object detection via grid cells. GANs: minmax V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))]." },  
  "Reinforcement Learning": { "quote": "Q-learning: Q(s,a) ← Q(s,a) + α[r + γmaxQ(s',a') - Q(s,a)]. Policy gradients: ∇θJ(θ) ≈ E[∇θ log π(a|s) * R]." },  
  "Transfer Learning": { "quote": "Fine-tuning pretrained models (ResNet, BERT): Freeze layers → Update head." },  
  "Explainable AI": { "quote": "SHAP values: ϕ_i = Σ_{S⊆N{i}} [|S|!(|N| - |S| - 1)! / |N|!] (f(S∪{i}) - f(S))." },  
  "AI Ethics": { "quote": "Bias mitigation: Disparate impact ratio = P(ŷ=1 | protected) / P(ŷ=1 | non-protected)." },  
  "AutoML": { "quote": "Hyperparameter tuning: Bayesian optimization → argmax_{θ} acquisition_function(θ)." },  
  "AI Hardware": { "quote": "TPUs: Optimized for matmul → Speedup training ResNet-50 by 10x vs. CPUs." },  
  "Federated Learning": { "quote": "Decentralized training: Δθ = ClientUpdate(data_i) → ServerAggregate(Δθ_1, ..., Δθ_n)." },  
  "AI Safety": { "quote": "Corrigibility: Reward H(s) = R(s) + β⋅KL[π||π_prior] (avoid side effects)." },  
  "Quantum Machine Learning": { "quote": "Grover’s algorithm: O(√N) search → Optimize QAOA for VQE." },  
  "AI in Healthcare": { "quote": "UNet: Medical image segmentation → Dice loss = 2|X∩Y| / (|X| + |Y|)." },  
  "Robotics & AI": { "quote": "SLAM: Simultaneous Localization and Mapping → Kalman filter updates." },  
  "AI for Games": { "quote": "Monte Carlo Tree Search (MCTS): Selection → Expansion → Simulation → Backpropagation (AlphaGo)." },  
  "Swarm Intelligence": { "quote": "Ant Colony Optimization: Δpheromone_ij = Q / path_length (short paths reinforced)." },  
  "AI Governance": { "quote": "EU AI Act: Risk tiers (unacceptable, high, limited) → Compliance via conformity assessments." },  
  "AI Creativity": { "quote": "GPT-3: p(x_t | x_{<t}) → Generate text via top-k sampling (temperature τ controls diversity)." },  
  "Edge AI": { "quote": "TinyML: Quantize model → 8-bit integers → Deploy on Arduino/Raspberry Pi." },  
  "AI for Climate": { "quote": "Optimize energy grids: RL agents → Minimize CO₂ g/kWh via renewable scheduling." },  
  "AI in Finance": { "quote": "LSTM: Predict stock trends via time-series data. Fraud detection: Autoencoders → Anomaly score = ||x - x̂||²." },  
  "AI in Education": { "quote": "Adaptive learning: Bayesian Knowledge Tracing → P(L_t) = P(L_{t-1}) + (1 - P(L_{t-1})) * slip." },  
  "AI for Art": { "quote": "StyleGAN: Latent space interpolation → z = αz₁ + (1-α)z₂ (morphing faces)." },  
  "AI & Law": { "quote": "Predictive policing audits: Ensure fairness Δ < 0.1 (equalized odds)." },  
  "Future of AI": { "quote": "AGI: Reward modeling → Recursive self-improvement (Yudkowsky’s coherence arguments)." }  
}  
