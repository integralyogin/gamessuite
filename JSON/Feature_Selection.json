{
  "Filter Methods": { "quote": "Selecting features based on statistical measures: Pearson's |r| > threshold." },
  "Wrapper Methods": { "quote": "Using model performance: RFE (Recursive Feature Elimination) prunes low-importance features." },
  "Embedded Methods": { "quote": "Algorithms with built-in selection: L1 regularization (Lasso) forces β_i = 0." },
  "Variance Threshold": { "quote": "Remove low-variance features: Var(X) < α (e.g., constants)." },
  "Mutual Information": { "quote": "I(X;Y) = ΣΣ p(x,y) log(p(x,y)/(p(x)p(y))) measures dependency." },
  "Chi-Square Test": { "quote": "χ² = Σ[(O - E)²/E] for categorical feature-target associations." },
  "ANOVA F-Value": { "quote": "F = (Between-group variance) / (Within-group variance) for numeric-categorical pairs." },
  "Correlation Analysis": { "quote": "Discard collinear features: |ρ(X_i, X_j)| > 0.8." },
  "L1 Regularization (Lasso)": { "quote": "β̂ = argmin(‖y - Xβ‖² + λ‖β‖₁) induces sparsity." },
  "Tree-Based Importance": { "quote": "Random Forest/Gini importance: ΣΔimpurity across splits." },
  "Permutation Importance": { "quote": "Δ accuracy when feature is shuffled (Breiman, 2001)." },
  "Forward Selection": { "quote": "Greedy additive approach: Add features until AUC plateaus." },
  "Backward Elimination": { "quote": "Greedy subtractive approach: Remove features with p > 0.05." },
  "PCA for Dimensionality Reduction": { "quote": "Project to eigenvectors: max Var(wᵀX) s.t. ‖w‖ = 1." },
  "LDA for Supervised Reduction": { "quote": "Maximize class separation: argmax (Between-class scatter)/(Within-class scatter)." },
  "Information Gain": { "quote": "IG(Y,X) = H(Y) - H(Y|X) for decision trees (e.g., C4.5)." },
  "Recursive Feature Elimination (RFE)": { "quote": "Iteratively remove worst-performing features (Guyon & Elisseeff, 2003)." },
  "Genetic Algorithms": { "quote": "Evolutionary search: Fitness = model accuracy, crossover/mutation for subsets." },
  "SHAP for Interpretability": { "quote": "φ_i = Σ[|S|!(|F|-|S|-1)!/|F|!] (f(S∪{i}) - f(S)) (Lundberg & Lee, 2017)." },
  "Variance Inflation Factor (VIF)": { "quote": "Detect multicollinearity: VIF = 1/(1 - R²) > 10 → remove." },
  "AutoML Tools": { "quote": "Automated selection: H2O, TPOT, or Boruta for optimal subsets." },
  "Domain Knowledge": { "quote": "Expert-driven selection: Clinical relevance > p-values in medical AI." },
  "Deep Feature Selection": { "quote": "Autoencoders: Minimize reconstruction loss ‖X - Decoder(Encoder(X))‖²." },
  "Stability Selection": { "quote": "Consensus across subsamples: Features chosen > 80% of iterations (Meinshausen & Bühlmann, 2010)." },
  "Elastic Net": { "quote": "L1 + L2 regularization: β̂ = argmin(‖y - Xβ‖² + λ₁‖β‖₁ + λ₂‖β‖₂²)." },
  "Minimum Redundancy Maximum Relevance (mRMR)": { "quote": "Max relevance I(X;Y), min redundancy I(X_i;X_j)." },
  "Feature Interaction Detection": { "quote": "Identify X_i * X_j via partial dependence plots (Friedman, 2001)." },
  "Causal Feature Selection": { "quote": "Granger causality: X causes Y if X improves Y's prediction (Granger, 1969)." },
  "Embedded Deep Learning": { "quote": "Attention mechanisms: Softmax( QKᵀ/√d ) highlights salient features (Vaswani et al., 2017)." }
}
