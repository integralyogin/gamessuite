{
  "Loss Minimization": { "quote": "Core objective: argmin_θ J(θ) where J(θ) = Loss + Regularization." },
  "Mean Squared Error (MSE)": { "quote": "Regression: MSE = (1/n)Σ(y_i - ŷ_i)², minimizing prediction variance." },
  "Cross-Entropy Loss": { "quote": "Classification: L = -Σ y_i log(ŷ_i), maximizing likelihood (log loss)." },
  "Hinge Loss": { "quote": "SVMs: L = max(0, 1 - yŷ), optimizing margin maximization." },
  "Regularization (L1/L2)": { "quote": "Prevent overfitting: L2 adds λΣθ², L1 adds λΣ|θ| (Tikhonov, 1943)." },
  "Dropout": { "quote": "Stochastic regularization: deactivate p% neurons per step (Srivastava et al., 2014)." },
  "Gradient Descent": { "quote": "Update rule: θ ← θ - η∇θJ(θ), where η is learning rate (Cauchy, 1847)." },
  "Stochastic GD (SGD)": { "quote": "Mini-batch updates: θ ← θ - η∇θJ(θ; batch), reducing variance." },
  "Adam Optimizer": { "quote": "Adaptive moments: m_t = β₁m_{t-1} + (1-β₁)g_t, ... (Kingma & Ba, 2015)." },
  "Learning Rate Scheduling": { "quote": "Dynamic η: Step decay (η_t = η₀ * γ^floor(t/k)) or cosine annealing." },
  "Early Stopping": { "quote": "Halt training when validation loss plateaus (Prechelt, 1998)." },
  "Batch Normalization": { "quote": "Stabilize training: x̂ = (x - μ)/σ, then scale/shift (Ioffe & Szegedy, 2015)." },
  "Gradient Clipping": { "quote": "Prevent exploding gradients: ||g|| ≤ c, common in RNNs." },
  "Weight Initialization": { "quote": "Xavier/Glorot: Var(w) = 2/(n_in + n_out); He: Var(w) = 2/n_in (2015)." },
  "Data Augmentation": { "quote": "Expand dataset: Rotate/flip images, add noise, or mix samples (e.g., MixUp)." },
  "Label Smoothing": { "quote": "Soft labels: y' = y(1 - α) + α/K, reducing overconfidence (Szegedy et al., 2016)." },
  "Momentum": { "quote": "v_t = γv_{t-1} + η∇J(θ); dampens oscillations (Polyak, 1964)." },
  "Knowledge Distillation": { "quote": "Student mimics teacher: L = αL_CE(y, ŷ) + (1-α)L_KL(p_teacher, p_student)." },
  "Multi-Task Learning": { "quote": "Shared representation: J_total = Σλ_i J_i, e.g., joint classification + detection." },
  "Autoencoder Objective": { "quote": "Reconstruction loss: L = ||x - decoder(encoder(x))||²." },
  "Contrastive Loss": { "quote": "Similarity learning: L = max(0, margin - d(pos) + d(neg)) (Hadsell et al., 2006)." },
  "Policy Gradient (RL)": { "quote": "Maximize expected reward: ∇θJ(θ) ≈ E[∇θ log π(a|s) Q(s,a)]." },
  "Triplet Loss": { "quote": "Anchor-positive-negative: L = max(0, d(a,p) - d(a,n) + margin) (FaceNet, 2015)." },
  "Domain Adaptation": { "quote": "Align source/target: min L_task + λL_MMD (Maximum Mean Discrepancy)." },
  "Sparse Coding": { "quote": "Minimize: ||x - Dα||² + λ||α||₁, learning sparse representations." },
  "Neural Architecture Search (NAS)": { "quote": "Optimize architecture: max_θ E[A(θ)] (Zoph & Le, 2016)." }
}
