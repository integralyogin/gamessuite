{
  "Input Layer": { "quote": "Raw data input: x ∈ ℝⁿ (e.g., pixels, tokens)." },
  "Dense Layer": { "quote": "Fully connected: y = σ(Wx + b), where σ is activation." },
  "Convolutional Layer": { "quote": "Kernel operations: (I ∗ K)[i,j] = ΣₘΣₙ I[i+m,j+n]K[m,n] (spatial feature extraction)." },
  "ReLU Activation": { "quote": "Non-linear activation: f(x) = max(0,x). Introduced by Nair & Hinton (2010)." },
  "Batch Normalization": { "quote": "Stabilize training: x̂ = (x - μ)/√(σ² + ε); y = γx̂ + β." },
  "Dropout": { "quote": "Regularization: During training, deactivate neurons with probability p (Srivastava et al., 2014)." },
  "Residual Connections": { "quote": "Skip connections: y = F(x) + x (mitigates vanishing gradients; He et al., 2016)." },
  "LSTM Layer": { "quote": "Gated memory: c_t = f ⊙ c_{t-1} + i ⊙ g (Hochreiter & Schmidhuber, 1997)." },
  "Encoder-Decoder": { "quote": "Sequence-to-sequence: Encoder compresses input, decoder generates output (Sutskever et al., 2014)." },
  "Transformer": { "quote": "Attention-based: Multi-head(Q,K,V) = Concat(head₁,…,headₖ)Wᴼ (Vaswani et al., 2017)." },
  "Self-Attention": { "quote": "Scaled dot-product: Attention(Q,K,V) = softmax(QKᵀ/√d)V (Vaswani et al., 2017)." },
  "Embedding Layer": { "quote": "Learned representations: E ∈ ℝ^{|V|×d} maps tokens to vectors (e.g., word2vec)." },
  "Pooling Layer": { "quote": "Dimensionality reduction: MaxPool(x)[i] = max(x[i×s : (i+1)×s])." },
  "Autoencoder": { "quote": "Bottleneck architecture: Minimize reconstruction loss ‖x - D(E(x))‖²." },
  "GAN Architecture": { "quote": "Adversarial training: min_G max_D V(D,G) = E[log D(x)] + E[log(1 - D(G(z)))]. (Goodfellow et al., 2014)." },
  "U-Net": { "quote": "Skip connections in encoder-decoder: Used for segmentation (Ronneberger et al., 2015)." },
  "Capsule Network": { "quote": "Vector capsules: Routing by agreement (Sabour et al., 2017)." },
  "Transformer-XL": { "quote": "Segment recurrence: Cache hidden states for long-range context (Dai et al., 2019)." },
  "Mixture of Experts": { "quote": "Sparse activation: y = Σᵢ gᵢ(x)Eᵢ(x), where gᵢ routes inputs (Shazeer et al., 2017)." },
  "Diffusion Models": { "quote": "Iterative denoising: x_{t-1} = μ_θ(x_t,t) + σ_tε (Ho et al., 2020)." },
  "Sparse Attention": { "quote": "Local + global patterns: Factorized attention (Child et al., 2019)." },
  "Perceiver IO": { "quote": "Cross-attention for modality fusion: Jaegle et al., 2021." },
  "Neural Turing Machine": { "quote": "Differentiable memory: Read/write with attention (Graves et al., 2014)." },
  "Vision Transformer": { "quote": "Image patches as tokens: x_p ∈ ℝ^{16×16×3} → linear projection → ViT (Dosovitskiy et al., 2020)." },
  "Adam Optimizer": { "quote": "Adaptive learning rates: m_t = β₁m_{t-1} + (1-β₁)g_t; v_t = β₂v_{t-1} + (1-β₂)g_t² (Kingma & Ba, 2015)." },
  "Softmax Layer": { "quote": "Output probabilities: p_i = e^{z_i} / Σⱼ e^{z_j}." },
  "Parameter-Efficient Fine-Tuning": { "quote": "LoRA: ΔW = BA, where B,A are low-rank (Hu et al., 2021)." },
  "Activation Maximization": { "quote": "Visualize features: x* = argmax_x f(x;θ)_i (Erhan et al., 2009)." },
  "Dynamic Computation Graphs": { "quote": "PyTorch/TensorFlow define forward pass programmatically (e.g., control flow ops)." },
  "Neural Architecture Search": { "quote": "Automated design: Search space + controller (Zoph & Le, 2017)." }
}
