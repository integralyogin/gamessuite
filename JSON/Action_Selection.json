{
  "Utility-Based Selection": { "quote": "Choose action a* = argmaxₐ EU(a), where EU(a) = Σ P(s|a) U(s)." },
  "Markov Decision Processes (MDPs)": { "quote": "Optimal policy π*(s) = argmaxₐ [R(s,a) + γ Σ T(s'|s,a) V(s')] (Bellman Equation)." },
  "Partially Observable MDPs (POMDPs)": { "quote": "Belief states b(s): a* = argmaxₐ Σₛ b(s) Q(s,a)." },
  "Q-Learning": { "quote": "Update rule: Q(s,a) ← Q(s,a) + α[r + γ maxₐ’ Q(s’,a’) - Q(s,a)] (Sutton & Barto, 1998)." },
  "Exploration vs Exploitation": { "quote": "Balancing novel actions (explore) vs known rewards (exploit): Upper Confidence Bound (UCB)." },
  "Greedy Action Selection": { "quote": "Always choose a* = argmaxₐ Q(s,a) → Risk of local optima." },
  "Epsilon-Greedy Strategy": { "quote": "With probability ε, explore randomly; else exploit (ε ∈ [0,1])." },
  "Thompson Sampling": { "quote": "Bayesian approach: Sample from posterior distributions to select actions." },
  "Multi-Armed Bandit Problem": { "quote": "Regret minimization: Minimize Σ(maxₐ μₐ - μₐₜ) over time T." },
  "Monte Carlo Tree Search (MCTS)": { "quote": "Selection → Expansion → Simulation → Backpropagation (e.g., AlphaGo)." },
  "Hierarchical Task Networks": { "quote": "Decompose goals into sub-actions: Plan(goal) → {action₁, action₂, ...}." },
  "BDI Architecture": { "quote": "Belief-Desire-Intention: Select actions aligning with agent’s goals (Bratman, 1987)." },
  "Real-Time Decision Making": { "quote": "Anytime algorithms: Sacrifice optimality for speed (e.g., Real-Time A*)." },
  "Game Theory & Nash Equilibrium": { "quote": "Select action a_i such that ∀i, π_i(a_i, a_{-i}) ≥ π_i(a'_i, a_{-i})." },
  "Action Selection in Swarms": { "quote": "Emergent decisions via local rules: Boids model (separation, alignment, cohesion)." },
  "Dynamic Programming": { "quote": "V(s) = maxₐ [R(s,a) + γ Σ T(s'|s,a) V(s')] (Value Iteration)." },
  "Policy Gradient Methods": { "quote": "∇θ J(θ) ≈ E[∇θ log π(a|s;θ) Q(s,a)] (REINFORCE algorithm)." },
  "Deep Q-Networks (DQN)": { "quote": "Q(s,a;θ) ≈ Q*(s,a), with experience replay and target networks (Mnih et al., 2015)." },
  "Risk-Sensitive Selection": { "quote": "Maximize CVaR (Conditional Value-at-Risk) over expected utility." },
  "Inverse Reinforcement Learning": { "quote": "Infer reward function R(s) from expert actions (Ng & Russell, 2000)." },
  "Ethical Action Selection": { "quote": "Constrained optimization: maxₐ U(a) s.t. moral constraints C(a)." },
  "Action Selection in Robotics": { "quote": "Sense → Plan → Act: ROS (Robot Operating System) pipelines." },
  "Action Pruning": { "quote": "Eliminate suboptimal actions early: Prune if Q(s,a) < threshold." },
  "Contextual Bandits": { "quote": "Actions depend on context x: a* = argmaxₐ f(x,a;θ) (e.g., LinUCB)." },
  "Action Selection in Games": { "quote": "Minimax: a* = argmaxₐ minₒ U(a,o) (opponent’s worst response)." },
  "Neuroevolution of Action Selection": { "quote": "Evolve neural network policies via genetic algorithms (Stanley & Miikkulainen, 2002)." },
  "Action Selection Under Uncertainty": { "quote": "Maximize expected utility: a* = argmaxₐ Σ P(ω) U(a,ω)." },
  "Multi-Objective Optimization": { "quote": "Pareto optimality: a* ∈ {a | ¬∃ a' s.t. ∀i f_i(a') ≥ f_i(a)}." },
  "Action Selection in NLP": { "quote": "Dialogue systems: Choose response maximizing coherence + user intent (e.g., Rasa)." },
  "Future Directions": { "quote": "Metacognition: Agents that adaptively switch selection strategies (e.g., meta-learning)." }
}
